apiVersion: argoproj.io/v1alpha1
kind: Sensor
metadata:
  name: webhook
spec:
  template:
    serviceAccountName: argo-events-sa 
  dependencies:
    - name: deploypysparkjob
      eventSourceName: webhook
      eventName: runcontainer
  triggers:
    - template:
        name: webhook-workflow-trigger
        k8s:
          group: argoproj.io
          version: v1alpha1
          resource: workflows
          operation: create
          source:
            resource:
              apiVersion: argoproj.io/v1alpha1
              kind: Workflow
              metadata:
                generateName: streamstatepy-  # Name of this Workflow
              spec:
                serviceAccountName: argo-events-sa #${runserviceaccount}  ## argoevents-runsa
                arguments:
                  parameters:
                    - name: pythoncode
                      value: hello world # this should be overridden
                    - name: inputs
                      value: hello world # this should be overridden
                    - name: assertions
                      value: hello world # this should be overridden
                entrypoint: main     
                volumeClaimTemplates: # do I need this?
                - metadata:
                    name: work
                  spec:
                    accessModes: [ "ReadWriteOnce" ]
                    resources:
                      requests:
                        storage: 64Mi
                templates:
                - name: main
                  inputs:
                    parameters:
                      - name: pythoncode
                      - name: inputs
                      - name: assertions
                  dag:
                    tasks:
                    - name: runmypy
                      template: mypy
                      arguments:
                        parameters: 
                         - name: pythoncode
                           value: "{{inputs.parameters.pythoncode}}"
                         
                    - name: rununittests
                      #dependencies: [runmypy]
                      template: unittests
                      arguments:
                        parameters: 
                         - name: pythoncode
                           value: "{{inputs.parameters.pythoncode}}"
                         - name: inputs
                           value: "{{inputs.parameters.inputs}}"
                         - name: assertions
                           value: "{{inputs.parameters.assertions}}"

                    - name: deployjob 
                      template: sparkjob
                      dependencies: [runmypy, rununittests]

                - name: mypy
                  inputs:
                    parameters:
                      - name: pythoncode
                  container:
                    volumeMounts: # may not need this
                    - mountPath: /work #this persists across steps
                      name: work
                    image: cytopia/mypy #${registryprefix}/${project}/${project}/scalacompile:v0.8.0
                    command: [sh, -c]
                    args: [
                      "echo {{inputs.parameters.pythoncode}} | base64 --decode > process.py;\
                      mypy process.py --ignore-missing-imports "
                    ]
                  archiveLocation: # maybe delete this...
                    archiveLogs: false

                - name: unittests
                  inputs:
                    parameters:
                      - name: pythoncode
                      - name: inputs
                      - name: assertions
                  #inputs:
                    #parameters:
                      # Name of the image to push
                      #- name: image
                      #  value: ${registryprefix}/${project}/${registry}/scalaapp:v0.1.0 # TODO!  specify tag version
                  #serviceAccountName: ${dockersecretwrite}
                  container:
                    image: unittestpy:v0.3.0
                    imagePullPolicy: Never
                    command: [sh, -c]
                    args: [
                      "echo $(pwd);\
                      echo {{inputs.parameters.pythoncode}} | base64 --decode > process.py;\
                      echo {{inputs.parameters.inputs}} | base64 --decode  > sampleinputs.json;\
                      echo {{inputs.parameters.assertions}} | base64 --decode > assertedoutputs.json;\
                      python3 streamstate/run_test.py process.py sampleinputs.json assertedoutputs.json"
                    ]
                    #workingDir: /work
                    volumeMounts:
                    - mountPath: /work 
                      name: work
                
                
                - name: build-docker-image
                  inputs:
                    parameters:
                      # Name of the image to push
                      - name: image
                        value: ${registryprefix}/${project}/${registry}/python_app:v0.1.0 # TODO!  specify tag version, different docker name per app
                  serviceAccountName: ${dockersecretwrite}
                  container:
                    image: gcr.io/kaniko-project/executor:latest
                    args:
                    - "--dockerfile=/work/Final.Dockerfile"
                    - "--context=dir:///work"
                    - "--destination={{inputs.parameters.image}}"
                    workingDir: /work
                    volumeMounts:
                    - mountPath: /work 
                      name: work

                - name: sparkjob ### todo deploy spark job using just created docker image
                  container:
                    image: alpine:3.7
                    command: [sh, -c, -x]
                    args: ['sleep 1; echo "got here"']

          parameters:
            - src:
                dependencyName: deploypysparkjob
                dataKey: body.pythoncode
              dest: spec.arguments.parameters.0.value
            - src:
                dependencyName: deploypysparkjob
                dataKey: body.inputs
              dest: spec.arguments.parameters.1.value
            - src:
                dependencyName: deploypysparkjob
                dataKey: body.assertions
              dest: spec.arguments.parameters.2.value