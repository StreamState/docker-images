apiVersion: argoproj.io/v1alpha1
kind: Sensor
metadata:
  name: streamstateapi
spec:
  template:
    serviceAccountName: argo-events-sa 
  dependencies:
    - name: deploypysparkjob
      eventSourceName: streamstatewebservice
      eventName: runcontainer
  triggers:
    - template:
        name: webhook-workflow-trigger
        k8s:
          group: argoproj.io
          version: v1alpha1
          resource: workflows
          operation: create
          source:
            resource:
              apiVersion: argoproj.io/v1alpha1
              kind: Workflow
              metadata:
                generateName: streamstatepy-  # Name of this Workflow
              spec:
                serviceAccountName: ${runserviceaccount} 
                arguments:
                  parameters:
                    - name: pythoncode
                      value: hello world # this should be overridden
                    - name: inputs
                      value: hello world # this should be overridden
                    - name: assertions
                      value: hello world # this should be overridden
                    - name: kafka
                      value: hello world # this should be overridden
                    - name: outputs
                      value: hello world # this should be overridden
                    - name: fileinfo
                      value: hello world # this should be overridden
                    - name: table
                      value: hello world # this should be overridden
                entrypoint: main     
                volumeClaimTemplates: # do I need this?
                - metadata:
                    name: work
                  spec:
                    accessModes: [ "ReadWriteOnce" ]
                    resources:
                      requests:
                        storage: 64Mi
                templates:
                - name: main
                  inputs:
                    parameters:
                      - name: pythoncode
                      - name: inputs
                      - name: assertions
                      - name: kafka
                      - name: outputs
                      - name: fileinfo
                      - name: table
                  dag:
                    tasks:
                    - name: runmypy
                      template: mypy
                      arguments:
                        parameters: 
                         - name: pythoncode
                           value: "{{inputs.parameters.pythoncode}}"
                         
                    - name: rununittests
                      #dependencies: [runmypy]
                      template: unittests
                      arguments:
                        parameters: 
                         - name: pythoncode
                           value: "{{inputs.parameters.pythoncode}}"
                         - name: inputs
                           value: "{{inputs.parameters.inputs}}"
                         - name: assertions
                           value: "{{inputs.parameters.assertions}}"

                    - name: runcassandra 
                      template: cassandra
                      arguments:
                        parameters:
                         - name: table
                           value: "{{inputs.parameters.table}}" # why does it error here??  could it be table is a reserved name?
                      dependencies: [runmypy, rununittests]


                    - name: builddocker
                      template: create-docker-file
                      dependencies: [runmypy, rununittests]

                    - name: deploydocker 
                      template: build-docker-image
                      dependencies: [builddocker]

                    - name: runspark
                      template: sparksubmit
                      arguments:
                        parameters: 
                         - name: pythoncode
                           value: "{{inputs.parameters.pythoncode}}"
                         - name: inputs
                           value: "{{inputs.parameters.inputs}}"
                         - name: kafka
                           value: "{{inputs.parameters.kafka}}"
                         - name: outputs
                           value: "{{inputs.parameters.outputs}}"
                         - name: fileinfo
                           value: "{{inputs.parameters.fileinfo}}"
                      dependencies: [deploydocker]
                    

                - name: mypy
                  inputs:
                    parameters:
                      - name: pythoncode
                  container:
                    volumeMounts: # may not need this
                    - mountPath: /work #this persists across steps
                      name: work
                    image: cytopia/mypy #${registryprefix}/${project}/${project}/scalacompile:v0.8.0
                    command: [sh, -c]
                    args: [
                      "echo {{inputs.parameters.pythoncode}} | base64 --decode > process.py;\
                      mypy process.py --ignore-missing-imports "
                    ]
                  archiveLocation: # maybe delete this...
                    archiveLogs: false

                - name: unittests
                  inputs:
                    parameters:
                      - name: pythoncode
                      - name: inputs
                      - name: assertions
                  container:
                    image: ${registryprefix}/${project}/${registry}/pysparkbase:v0.1.0
                    imagePullPolicy: IfNotPresent
                    command: [sh, -c]
                    args: [
                      "echo $(pwd);\
                      echo {{inputs.parameters.pythoncode}} | base64 --decode > process.py;\
                      echo {{inputs.parameters.inputs}} > sampleinputs.json;\
                      echo {{inputs.parameters.assertions}} > assertedoutputs.json;\
                      run-test process.py sampleinputs.json assertedoutputs.json
                      "
                    ]
                    workingDir: /work
                    volumeMounts:
                    - mountPath: /work 
                      name: work

                - name: create-docker-file
                  inputs:
                    parameters:
                      - name: dockerfileline1 
                        value: FROM ${registryprefix}/${project}/${project}/pysparkbase:v0.1.0
                      - name: dockerfileline2 
                        value: COPY process.py /opt/spark/work-dir/process.py
                  container:
                    image: cytopia/mypy # doesn't really matter, so may as well use an image already downloaded
                    imagePullPolicy: IfNotPresent
                    command: [sh, -c]
                    args: [
                      "echo {{inputs.parameters.dockerfileline1}} > /work/Final.Dockerfile;\
                      echo {{inputs.parameters.dockerfileline2}} >> /work/Final.Dockerfile;
                      "
                    ]
                    workingDir: /work
                    volumeMounts:
                    - mountPath: /work 
                      name: work
                
                - name: build-docker-image
                  inputs:
                    parameters:
                      # Name of the image to push
                      - name: image
                        value: ${registryprefix}/${project}/${registry}/python_app:v0.1.0 # TODO!  specify tag version, different docker name per app
                  serviceAccountName: ${dockersecretwrite}
                  container:
                    image: gcr.io/kaniko-project/executor:latest
                    args:
                    - "--dockerfile=/work/Final.Dockerfile"
                    - "--context=dir:///work"
                    - "--destination={{inputs.parameters.image}}"
                    workingDir: /work
                    volumeMounts:
                    - mountPath: /work 
                      name: work

                - name: cassandra
                  inputs:
                    parameters:
                      - name: table
                  container:
                    image: ${registryprefix}/${project}/${registry}/cassandrapy:v0.1.0 
                    command: [sh, -c]
                    args: [
                      "python3 entrypoint.py {{inputs.parameters.table}};" #
                    ]
                  envFrom:
                    - secretRef:
                        name: ${cassandrasecret} # add this from terraform
                    - configMapRef:
                        name: ${dataconfig} # add this from terraform

                  #todo, loop this for each topic.  see, eg, https://github.com/argoproj/argo-workflows/blob/master/examples/loops.yaml
                - name: sparksubmit
                  inputs:
                    parameters:
                      # Name of the image to push
                      - name: image
                        value: ${registryprefix}/${project}/${registry}/python_app:v0.1.0 # same as for build-docker-image
                      #- name: pythoncode
                      - name: inputs
                      - name: kafka
                      - name: outputs
                      - name: fileinfo
                  resource:                   # indicates that this is a resource template
                    action: apply            # can be any kubectl action (e.g. create, delete, apply, patch)
                    # The successCondition and failureCondition are optional expressions.
                    # If failureCondition is true, the step is considered failed.
                    # If successCondition is true, the step is considered successful.
                    # They use kubernetes label selection syntax and can be applied against any field
                    # of the resource (not just labels). Multiple AND conditions can be represented by comma
                    # delimited expressions.
                    # For more details: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
                    successCondition: status.succeeded > 0
                    failureCondition: status.failed > 3 
                    manifest: |               #put your kubernetes spec here
                      apiVersion: "sparkoperator.k8s.io/v1beta2"
                      kind: SparkApplication
                      metadata:
                        name: testdev #todo, make this a function of "topics" (ie, the thing to be looping over)
                        namespace: ${namespace} # need to add this to terraform
                      spec:
                        sparkConf:
                          spark.jars.packages: "org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0"
                          spark.jars.repositories: "https://packages.confluent.io/maven"
                          spark.jars.ivy: "/tmp/ivy"
                        type: Python
                        pythonVersion: "3"
                        mode: cluster
                        image: {{inputs.parameters.image}}
                        imagePullPolicy: Always
                        mainApplicationFile: "local:///opt/spark/work-dir/dev_app.py" # todo, adjust this to be the right entrypoint
                        sparkVersion: "3.1.1"
                        hadoopConf:
                          "fs.gs.project.id": ${project}
                          "fs.gs.system.bucket": "streamstate-sparkstorage-${organization}"  
                          "google.cloud.auth.service.account.enable": "true" 
                          "google.cloud.auth.service.account.json.keyfile": "/mnt/secrets/key.json"
                          "fs.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"
                          "fs.AbstractFileSystem.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"
                        
                        arguments:
                          - testdev 
                            # todo, this should be same as metadata: name
                          - {{inputs.parameters.outputs}}
                          - {{inputs.parameters.fileinfo}}
                          - {{inputs.parameters.inputs}}
                          # todo, need to pass in version from output of cassandra step
                        
                          #volumes:
                        #  - name: spark-data
                        #    persistentVolumeClaim:
                        #      claimName: spark-data
                        restartPolicy:
                          type: Always # should be able to resume from checkpoint if killed for some reason
                        driver:
                          coreRequest: 200m
                          memory: "512m"
                          serviceAccount: spark # this maps to spark-gcs 
                          #volumeMounts:
                          #  - name: spark-data
                          #    mountPath: /mnt/spark/work


                          metadata:
                            annotations:
                              prometheus.io/scrape: "true"
                              prometheus.io/path: "/metrics"
                              prometheus.io/port: "8090"
                          secrets:
                          - name: "spark-secret" # Secret name as defined when creating a kubernetes secret, generated from spark-gcs
                            path: "/mnt/secrets"
                            secretType: GCPServiceAccount
                          envFrom:
                            - secretRef:
                                name: ${cassandrasecret} # add this from terraform
                            - configMapRef:
                                name: ${dataconfig} # add this from terraform
                          #env:
                            #- name: CASSANDRA_LOADBALANCER_SERVICE_HOST
                            #  valueFrom:
                            #    configMapKeyRef:
                            #      name: cassandra-data
                            #      key: clusterName
                            #- name: CASSANDRA_LOADBALANCER_SERVICE_HOST # TODO! pass this in as part of json
                            #  value: cluster1-dc1-service # todo!  this could possibly change if multiple cassandra racks/nodes/clusters
                            #- name: CASSANDRA_LOADBALANCER_SERVICE_PORT
                            #  value: "9042"
                        executor:
                          instances: 1
                          cores: 1
                          memory: "512m"
                          #volumeMounts:
                          #  - name: spark-data
                          #    mountPath: /mnt/spark/work
                          secrets:
                          - name: "spark-secret" # Secret name as defined when creating a kubernetes secret
                            path: "/mnt/secrets"
                            secretType: GCPServiceAccount
                        monitoring: 
                          exposeDriverMetrics: true
                          exposeExecutorMetrics: true
                          prometheus:
                            jmxExporterJar: "/prometheus/jmx_prometheus_javaagent-0.11.0.jar"
                            port: 8090
                        
    
          parameters:
            - src:
                dependencyName: deploypysparkjob
                dataKey: body.pythoncode # base 64 python code
              dest: spec.arguments.parameters.0.value
            - src:
                dependencyName: deploypysparkjob
                dataKey: body.inputs # list of topic, schema (fields: list), sample (list of inputs, eg [{"field1": "hello"}])
              dest: spec.arguments.parameters.1.value
            - src:
                dependencyName: deploypysparkjob
                dataKey: body.assertions # list of expected (eg, [{"field1: "hello", "field2":"hi"}])
              dest: spec.arguments.parameters.2.value
            - src:
                dependencyName: deploypysparkjob
                dataKey: body.kafka #brokers, as string (more to come....)
              dest: spec.arguments.parameters.3.value
            - src:
                dependencyName: deploypysparkjob
                dataKey: body.outputs # mode, checkpoint_location, output_name (which is probably just app name?)
              dest: spec.arguments.parameters.4.value
            - src:
                dependencyName: deploypysparkjob # todo, may not really need this if we fix https://github.com/StreamState/k8s_poc/issues/35
                dataKey: body.fileinfo # max_file_age
              dest: spec.arguments.parameters.5.value
            - src:
                dependencyName: deploypysparkjob
                dataKey: body.table # primary_keys: list, output_schema: avro
              dest: spec.arguments.parameters.6.value