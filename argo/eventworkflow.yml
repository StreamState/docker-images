apiVersion: argoproj.io/v1alpha1
kind: Sensor
metadata:
  name: webhook
spec:
  template:
    serviceAccountName: argo-events-sa 
  dependencies:
    - name: createsparkcontainerdeployment
      eventSourceName: webhook
      eventName: runcontainer
  triggers:
    - template:
        name: webhook-workflow-trigger
        k8s:
          group: argoproj.io
          version: v1alpha1
          resource: workflows
          operation: create
          source:
            resource:
              apiVersion: argoproj.io/v1alpha1
              kind: Workflow
              metadata:
                generateName: spark-streaming-ci-  # Name of this Workflow
              spec:
                serviceAccountName: argoevents-runsa
                arguments:
                  parameters:
                    - name: scalacode
                      value: hello world # this should be overridden
                entrypoint: main     
                volumeClaimTemplates:
                - metadata:
                    name: work
                  spec:
                    accessModes: [ "ReadWriteOnce" ]
                    resources:
                      requests:
                        storage: 64Mi
                templates:
                - name: main
                  inputs:
                    parameters:
                      - name: scalacode
                  dag:
                    tasks:
                    - name: A
                      template: build-jar
                      arguments:
                        parameters: 
                         - name: scalacode
                           value: "{{inputs.parameters.scalacode}}"
                    - name: B
                      dependencies: [A]
                      template: build-docker-image

                - name: build-jar     
                  inputs:
                    parameters:
                      - name: scalacode
                      - name: dockerfileline1 
                        value: FROM ${registryprefix}/${project}/${project}/sparkbase:v0.1.0
                      - name: dockerfileline2 
                        value: COPY streamstate.jar /opt/spark/work-dir/streamstate.jar
                  container:
                    volumeMounts:
                    - mountPath: /work #this persists across steps
                      name: work
                    image: ${registryprefix}/${project}/${project}/scalacompile:v0.5.0
                    command: [sh, -c]
                    args: [
                      "echo {{inputs.parameters.scalacode}} | base64 --decode > src/main/scala/custom.scala;\
                      sbt assembly; cp streamstate.jar /work/streamstate.jar; ls -la /work;\
                      echo {{inputs.parameters.dockerfileline1}} > /work/Final.Dockerfile;\
                      echo {{inputs.parameters.dockerfileline2}} >> /work/Final.Dockerfile;"
                    ]
                    #workingDir: /work
                  archiveLocation: # maybe delete this...
                    archiveLogs: false

                - name: build-docker-image
                  inputs:
                    parameters:
                      # Name of the image to push
                      - name: image
                        value: ${registryprefix}/${project}/${registry}/scalaapp:v0.1.0 # TODO!  specify tag version
                  serviceAccountName: ${dockersecretwrite}
                  container:
                    image: gcr.io/kaniko-project/executor:latest
                    args:
                    - "--dockerfile=/work/Final.Dockerfile"
                    - "--context=dir:///work"
                    - "--destination={{inputs.parameters.image}}"
                    workingDir: /work
                    volumeMounts:
                    - mountPath: /work 
                      name: work
          parameters:
            - src:
                dependencyName: createsparkcontainerdeployment
                dataKey: body.scalacode
              dest: spec.arguments.parameters.0.value
