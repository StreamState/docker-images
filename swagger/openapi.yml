swagger: "2.0"
info:
  description: "Streamstate endpoints"
  title: "Streamstate"
  version: "1.0.0"
host: ${HOST} # "echo-api.endpoints.YOUR-PROJECT-ID.cloud.goog"
x-google-endpoints:
- name: ${HOST}
  target: ${CLUSTER_IP}

schemes:
  - http

## todo, add firebase auth

security:
- api_key: []
securityDefinitions:
  # This section configures basic authentication with an API key.
  api_key:
    type: "apiKey"
    name: "key"
    in: "query"


paths:
  "/build/container":
    post:
      consumes:
      - application/json
      operationId: createSparkStreamingJob
      summary: Creates a spark streaming job 
      description: Runs unit tests and linting, starts a spark streaming job
      responses:
        '200':
          description: SuccessResponse
          schema:
            type: string
          examples:
            text/plain:
              "success"
        '400':
          description: Error for incorrect request
          schema:
            type: object
            properties: 
              err: 
                type: string
      parameters:
      - required: true
        name: body
        in: body
        schema:
          type: object
          properties:
            pythoncode: 
              type: string
              example: "base 64 encoded python code"
            inputs:
              type: array
              items:
                type: object
                properties:
                  topic: 
                    type: string
                    example: "topic1"
                  schema: # schema of the kafka topic
                    type: object
                    properties:
                      fields:
                        type: array
                        items:
                          type: object
                          properties:
                            name:
                              type: string
                              example: "field1"
                            type:
                              type: string
                              example: "string"   
                          required:
                          - name
                          - type          
                  sample: # samples from kafka topic
                    type: array
                    items:
                      type: object # variable properties.  An example is "field1": "somevalue"
                      properties:
                        field1: # not required, just an example
                          type: string 
                          example: "somevalue"
                      additionalProperties: true
                required:
                - topic
                - schema
                - sample

            assertions: # expected output from spark streaming job for unit testing
              type: array
              items:
                type: object
                properties:
                  field1: # not required, just an example
                    type: string 
                    example: "somevalue"
                additionalProperties: true
            kafka:
              type: object
              properties:
                brokers:
                  type: string
                  example: "broker1,broker2" # comma delineated
              required:
              - brokers
            outputs:
              type: object
              properties:
                mode: 
                  type: string
                  enum: [append, complete, update]
                checkpoint_location: # this should be moved to terraform at some point, when we point to gcs rather than local to docker
                  type: string
                  example: "/tmp/checkpoint"
                output_name: # This is the output topic.  Should this be the same as the cassandra table and the output_schema name??
                  type: string 
                
              required:
                - mode
                - checkpoint_location
                - output_name
            fileinfo:
              type: object
              properties:
                max_file_age:
                  type: string
                  example: "2d"
              required:
                - max_file_age
            table:
              type: object
              properties:
                primary_keys:
                  type: array
                  items:
                    type: string
                    example: "field1"
                output_schema: # avro schema
                  type: object
                  properties:
                    name: 
                      type: string # TODO THIS IS WRONG! avro schema requires it, and I currently use it to create the cassandra table name in entrypoint.  However I use the CassandraOutputStruct in spark streaming app to call the table, which uses the top level appname
                      example: myappname
                    fields:
                      type: array
                      items:
                        type: object
                        properties:
                          name: 
                            type: string
                            example: "field1"
                          type:
                            type: string
                            example: "string"
                        required:
                          - name
                          - type
                  required:
                    - name
                    - fields
            appname: 
              type: string
              example: mytestapp
          required:
           - pythoncode
           - inputs
           - assertions
           - kafka
           - outputs
           - fileinfo
           - table
           - appname
